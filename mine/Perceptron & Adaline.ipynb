{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perceptron class "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the original Perceptron Class provided by the professor, our goal is to implement a new class adaline, and modify the corresponding fi method to implement the adaline learning rule and test it on the iris data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Perceptron:\n",
    "    def __init__(self, eta):\n",
    "        self.w = None\n",
    "        self.b = None\n",
    "        self.eta = eta\n",
    "\n",
    "    # Heaviside step activation function\n",
    "    def activation(self, z):\n",
    "        return np.heaviside(z, 0) # Returns 0 if z ≤ 0, else 1\n",
    "\n",
    "    # Fit method to train the perceptron model\n",
    "    def fit(self, X, y, epochs):\n",
    "        n_features = X.shape[1]\n",
    "        n_samples = X.shape[0]\n",
    "\n",
    "        # Initializing weights, bias and predictions\n",
    "        self.w = np.random.rand(n_features)\n",
    "        self.b = 0\n",
    "        y_pred = np.zeros(n_samples)\n",
    "\n",
    "        # Iterating until the indicated number of epochs\n",
    "        for epoch in range(epochs):\n",
    "            change=0\n",
    "            for i in range(n_samples):\n",
    "                y_old = y_pred[i]\n",
    "                # Computing the dot product between sample and weights and adding the bias:\n",
    "                z = np.dot(X[i], self.w) + self.b \n",
    "                y_pred[i] = self.activation(z) # Passing through an activation function\n",
    "                if y_old != y_pred[i]:\n",
    "                    change += 1\n",
    "                # Updating weights and bias using the error\n",
    "                self.w = self.w + self.eta * (y[i] - y_pred[i]) * X[i]\n",
    "                self.b = self.b + self.eta * (y[i] - y_pred[i])\n",
    "            print(f\"\\t Epoch: {epoch +1} with {change} changes.\")\n",
    "            if not change:\n",
    "                print(f\"No changes. The perceptron model converged.\")\n",
    "                break\n",
    "\n",
    "    def predict(self, X):\n",
    "        z = np.dot(X, self.w) + self.b\n",
    "        return self.activation(z)\n",
    "\n",
    "    def get_params(self):\n",
    "        return self.w, self.b\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adaline Class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're supposed to implement the learning rule of the adaline over the Perceptron class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Adaline(Perceptron):\n",
    "    def fit(self, X, y, epochs):\n",
    "        n_features = X.shape[1]\n",
    "        n_samples = X.shape[0]\n",
    "\n",
    "        # Initializing weights, bias\n",
    "        self.w = np.random.rand(n_features)\n",
    "        self.b = 0\n",
    "\n",
    "        # Iterating until the indicated number of epochs\n",
    "        for epoch in range(epochs):\n",
    "            cost = 0\n",
    "            for i in range(n_samples):\n",
    "                # Computing the dot product between sample and weights and adding the bias:\n",
    "                z = np.dot(X[i], self.w) + self.b\n",
    "                # Error: difference between the true label and the predicted label\n",
    "                error = y[i] - z\n",
    "                \n",
    "                # Updating weights and bias using the error\n",
    "                self.w = self.w + self.eta * error * X[i]\n",
    "                self.b = self.b + self.eta * error\n",
    "                \n",
    "                # Accumulate cost\n",
    "                cost += 0.5 * error**2\n",
    "                if cost == 0:\n",
    "                    print(f\"No changes. The Adaline model converged.\")\n",
    "            print(f\"\\t Epoch: {epoch + 1} - Cost: {cost:.4f}\")\n",
    "    \n",
    "    def predict(self, X):\n",
    "        # In ADALINE, your model is trained by minimizing MSE between the raw output z and the true label y.\n",
    "        # That means:\n",
    "        #     •\tThe model is trying to make z=0 when the true label is 0\n",
    "        #     •\tAnd z=1 when the true label is 1\n",
    "        # That's why we need to change the threshold to 0.5\n",
    "        z = np.dot(X, self.w) + self.b\n",
    "        return (z >= 0.5).astype(int)  # Threshold at 0.5 instead of 0 like in the perceptron\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Iris Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's run our custom models with the iris dataset to see how it performs and to experiment the different parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "iris = load_iris()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's reduce the features to two, and make it a binary classification problem, so we classify one vs all."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = iris.data[:, (0,1)] # Let's reduce the dimensionality to 2\n",
    "y = (iris.target == 0).astype(int) # Let's make it a binary classification problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's scale and split the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, \n",
    "                                                shuffle=True, stratify=y)\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's initialize and train the perceptron model and see the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t Epoch: 1 with 46 changes.\n",
      "\t Epoch: 2 with 6 changes.\n",
      "\t Epoch: 3 with 1 changes.\n",
      "\t Epoch: 4 with 2 changes.\n",
      "\t Epoch: 5 with 0 changes.\n",
      "No changes. The perceptron model converged.\n",
      "model weights:[-3.11008597  1.64546991], model bias:-2.0\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(seed=42) # For reproducibility\n",
    "\n",
    "# Initialize the perceptron model with learning rate η=1\n",
    "model = Perceptron(1)\n",
    "\n",
    "model.fit(X_train, y_train, 5)\n",
    "w, b = model.get_params()\n",
    "print(f\"model weights:{w}, model bias:{b}\")\n",
    "\n",
    "y_train_predicted = model.predict(X_train)\n",
    "y_test_predicted = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train accuracy:1.0\n",
      "test accuracy:1.0\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "print(f\"train accuracy:{accuracy_score(y_train, y_train_predicted)}\")\n",
    "print(f\"test accuracy:{accuracy_score(y_test, y_test_predicted)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's test it on the Adaline Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t Epoch: 1 - Cost: 28.9331\n",
      "\t Epoch: 2 - Cost: 5.7398\n",
      "\t Epoch: 3 - Cost: 3.6971\n",
      "\t Epoch: 4 - Cost: 3.4944\n",
      "\t Epoch: 5 - Cost: 3.4680\n",
      "\t Epoch: 6 - Cost: 3.4630\n",
      "\t Epoch: 7 - Cost: 3.4617\n",
      "\t Epoch: 8 - Cost: 3.4614\n",
      "\t Epoch: 9 - Cost: 3.4613\n",
      "\t Epoch: 10 - Cost: 3.4612\n",
      "model weights:[-0.29250365  0.24496291], model bias:0.3408293533098461\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(seed=42) # For reproducibility\n",
    "\n",
    "# Initialize the perceptron model with learning rate η=0.6\n",
    "model = Adaline(0.01)\n",
    "\n",
    "model.fit(X_train, y_train, 10)\n",
    "w, b = model.get_params()\n",
    "print(f\"model weights:{w}, model bias:{b}\")\n",
    "\n",
    "y_train_predicted = model.predict(X_train)\n",
    "y_test_predicted = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train accuracy:0.9925925925925926\n",
      "test accuracy:1.0\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "print(f\"train accuracy:{accuracy_score(y_train, y_train_predicted)}\")\n",
    "print(f\"test accuracy:{accuracy_score(y_test, y_test_predicted)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Observed results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The main difference with the Perceptron, is that the Adaline does not use the heaviside step activation function in the training, but it calculates the error and tries to minimize it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To create the Adaline class we had to modify the fit and predict methods to update what was said before. The results are that Adaline seems more sensitive to the learning rate changes. When looking up or asking LLMs they indeed tell me:\n",
    "\n",
    "\"In Adaline, the learning rate becomes more critical because the weight updates are proportional to the gradient of the error function. You need to ensure the learning rate is chosen carefully to avoid slow convergence or instability.\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
